# Taller Pr谩ctico #2

## Caso de Estudio: Optimizaci贸n de la Atenci贸n al Cliente en una Empresa de E-commerce - Implementaci贸n de un Sistema RAG para IA Generativa

El objetivo de este taller es analizar un caso de estudio enfocado en la implementaci贸n de un sistema RAG (Retrieval-Augmented Generation) para optimizar la atenci贸n al cliente de una empresa de comercio electr贸nico.

En este caso, EcoMarket busca superar las limitaciones de los modelos de lenguaje de prop贸sito general, los cuales tienden a generar respuestas incompletas o imprecisas al no contar con informaci贸n espec铆fica y actualizada de la compa帽铆a. La idea es integrar un mecanismo de recuperaci贸n de documentos internos que permita que las respuestas sean precisas, coherentes y fundamentadas en datos reales de la organizaci贸n. De esta manera, el sistema podr谩 atender consultas frecuentes relacionadas con pedidos, devoluciones, inventario y pol铆ticas de la empresa, reduciendo los tiempos de respuesta, mejorando la experiencia del cliente y fortaleciendo la confianza en el canal de soporte automatizado.

---

## Fase 1: Selecci贸n de Componentes Clave del Sistema RAG

Para la implementaci贸n del sistema RAG en EcoMarket es necesario seleccionar cuidadosamente los componentes que permitir谩n garantizar precisi贸n, escalabilidad y eficiencia en el manejo de consultas de los clientes. En cuanto al modelo de embeddings, se opta por utilizar **Sentence Transformers multiling眉es de Hugging Face**, como *paraphrase-multilingual-MiniLM-L12-v2*, ya que ofrecen un buen equilibrio entre costo y rendimiento. Estos modelos son de c贸digo abierto, funcionan de manera eficiente en el idioma espa帽ol y pueden ejecutarse localmente, lo que evita la dependencia exclusiva de servicios propietarios y reduce gastos operativos. Esta elecci贸n asegura que la representaci贸n sem谩ntica de los documentos de la empresa sea adecuada para realizar b煤squedas precisas y r谩pidas en el sistema de recuperaci贸n.

Respecto a la base de datos vectorial, la propuesta inicial es utilizar **ChromaDB**, dado que es open-source, ligera y de f谩cil integraci贸n con frameworks como LangChain. Esto facilita su uso en un entorno acad茅mico o de prototipado, donde los recursos suelen ser limitados. Sin embargo, tambi茅n se contempla la posibilidad de migrar a una soluci贸n m谩s robusta como **Pinecone** en escenarios de producci贸n, debido a su capacidad de manejar millones de vectores, escalabilidad en la nube y tiempos de respuesta 贸ptimos. Aunque Pinecone implica un costo adicional, su facilidad de uso y soporte en entornos empresariales lo convierten en una opci贸n viable a futuro. Con esta combinaci贸n de componentes se logra un sistema flexible que puede adaptarse tanto a un entorno de pruebas como a uno de despliegue real, respondiendo a las necesidades de EcoMarket de contar con un servicio de atenci贸n al cliente confiable, actualizado y eficiente.

Perfecto , sigamos con la **Fase 2: Creaci贸n de la Base de Conocimiento**. Te lo redacto en forma de explicaci贸n narrativa (como en tu Lab 1), para que quede listo en el README del Taller 2:

---

## Fase 2: Creaci贸n de la Base de Conocimiento de Documentos

El 茅xito de un sistema RAG depende directamente de la calidad y organizaci贸n de la informaci贸n que se pone a disposici贸n del modelo. En el caso de EcoMarket, resulta fundamental construir una base de conocimiento que contenga los documentos m谩s relevantes para el proceso de atenci贸n al cliente. Para este prop贸sito, se identifican tres fuentes clave: **la pol铆tica de devoluciones y garant铆as en formato PDF**, ya que representa uno de los temas m谩s consultados por los clientes; **un archivo de inventario de productos en Excel o CSV**, que permite acceder de manera actualizada a la disponibilidad, precios y caracter铆sticas de los art铆culos; y **un documento JSON con preguntas frecuentes (FAQ)**, que recopila respuestas a dudas comunes en torno a env铆os, pagos y procesos de compra. Estos documentos constituyen la base m铆nima que garantizar谩 que el asistente pueda ofrecer respuestas fundamentadas y alineadas con la informaci贸n oficial de la empresa.

Para asegurar un buen desempe帽o en la b煤squeda sem谩ntica, es necesario dividir cada documento en fragmentos o *chunks* que puedan ser procesados por el modelo de embeddings. En este caso, se propone aplicar una **estrategia de segmentaci贸n recursiva**, que combina la separaci贸n por secciones naturales (como t铆tulos y p谩rrafos) con un control de tama帽o m谩ximo en tokens (ejemplo: 500 tokens con un solapamiento de 50). Este enfoque resulta m谩s adecuado que una segmentaci贸n fija, ya que evita romper frases o apartados importantes y mantiene la coherencia del contenido. Finalmente, cada fragmento ser谩 convertido en un vector utilizando el modelo de embeddings seleccionado y cargado en la base de datos vectorial. De esta manera, cuando un cliente formule una pregunta, el sistema podr谩 recuperar los fragmentos m谩s relevantes y construir una respuesta precisa y contextualizada.

---




